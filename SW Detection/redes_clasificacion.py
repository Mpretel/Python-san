#%%
# -*- coding: utf-8 -*-
"""Copia de Clase_3_Redes_Clasificacion_BIS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H5dldOgPLYPwORp9I90EQPSgKJmDRABX
"""

#!pip install tensorflow
#!pip install keras
#!pip install pydot
#!pip install graphviz

"""# Redes Neuronales

En este Notebook vamos a parender a crear y entrenar redes neuronales usando la librería **Keras**. En la primer sección vamos a resolver un problema de clasificación de imágenes y en la segunda sección un problema de regresión.

### Imports

Primero importemos las librerias que vamos a usar:
"""

# Imports de utilidades de Python
import numpy as np
import matplotlib.pyplot as plt
import sklearn.metrics as metrics

# Imports de Keras
from keras.datasets import mnist,fashion_mnist
from keras.models import Sequential, load_model
from keras.layers.core import Dense, Dropout, Activation
from keras.utils import np_utils, plot_model
from keras.regularizers import l1

# let's keep our keras backend tensorflow quiet
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='3'

"""### Preparando el dataset  -  Fashion MNIST
Vamos a trabajar usando un dataset de imágenes de prendas de ropa en baja resolución llamado fashion MNIST. Consiste en imagenes de 28x28 pixles en escala de grises de diez clases distintas de ropa. Para mas datos al respecto, peueden consultar la [documentacion de keras](https://keras.io/api/datasets/fashion_mnist/).

Primero importamos el dataset:
"""
#%%
(X_train_raw, y_train), (X_test_raw, y_test) = fashion_mnist.load_data()

categories = ['T-shirt/top', 'Trouser','Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']



#%%
"""Exploremos el tamaño del dataset. Noten que las imágenes poseen una resolución de 28 x 28 píxeles."""

# let's print the shape before we reshape and normalize
print("X_train shape:", X_train_raw.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test_raw.shape)
print("y_test shape:", y_test.shape)


#%%
"""Veamos como se ven algunas de las imágenes y que etiquetas les corresponden."""

fig = plt.figure(figsize = (12,12))
j = np.random.randint(0, len(X_train_raw))
for i in range(25):
    j = np.random.randint(0, len(X_train_raw))
    plt.subplot(5,5,i+1)
    plt.tight_layout()
    plt.imshow(X_train_raw[j], cmap='gray', interpolation='none')
    plt.title("Clase: {}".format(categories[y_train[j]]))
    plt.xticks([])
    plt.yticks([])
plt.show()

#%%
"""¿Cómo es la distribución de valores en la imagen?"""

fig = plt.figure(figsize = (7,7))
plt.subplot(2,1,1)
plt.imshow(X_train_raw[0], cmap='gray', interpolation='none')
plt.title("Digit: {}".format(categories[y_train[0]]))
plt.xticks([])
plt.yticks([])
plt.subplot(2,1,2)
plt.hist(X_train_raw[0].reshape(784))
plt.title("Pixel Value Distribution")
plt.show()

#%%
"""Preparamos los datos para entrar al modelo."""

X_train_raw_filt = []
y_train_filt = []
for i in range(len(y_train)):
  if y_train[i]==1:
    y_train_filt.append(1)
    X_train_raw_filt.append(X_train_raw[i])
  elif y_train[i]==2:
    y_train_filt.append(2)
    X_train_raw_filt.append(X_train_raw[i])

X_test_raw_filt = []
y_test_filt = []
for i in range(len(y_test)):
  if y_test[i]==1:
    y_test_filt.append(1)
    X_test_raw_filt.append(X_test_raw[i])
  elif y_test[i]==2:
    y_test_filt.append(2)
    X_test_raw_filt.append(X_test_raw[i])

X_train_raw_filt = np.array(X_train_raw_filt)
y_train_filt = np.array(y_train_filt)
X_test_raw_filt = np.array(X_test_raw_filt)
y_test_filt = np.array(y_test_filt)

print(X_train_raw_filt.shape)
print(y_train_filt.shape)
print(X_test_raw_filt.shape)
print(y_test_filt.shape)

#%%
X_train_raw = X_train_raw_filt[0:150]
y_train = y_train_filt[0:150]
y_test = y_test_filt[0:50]
X_test_raw = X_test_raw_filt[0:50]

# building the input vector from the 28x28 pixels
X_train_raw = X_train_raw.reshape(150, 784)
X_test_raw = X_test_raw.reshape(50, 784)
X_train_raw = X_train_raw.astype('float32')
X_test_raw = X_test_raw.astype('float32')

# normalizing the data to help with the training
X_train = X_train_raw/255.0
X_test = X_test_raw/255.0

# print the final input shape ready for training
print("X_train shape:", X_train_raw.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test_raw.shape)
print("y_test shape:", y_test.shape)
#%%
"""Veamos la variable ```y```, cuales son los valores posibles que puede tomar cuantas veces toma cada uno."""

values,counts = np.unique(y_train, return_counts=True)
print("Values:", values)
print("Counts:", counts)
suma = np.sum(counts)
print(suma)

#%%
"""Para entrenar la red neuronal, vamos convertir la variable `y` 
que consiste en un vector de largo 60000 que puede tomar 10 valores 
distintos a una nueva variabvle `Y` la cual consiste en una matriz 
de 60000x10 donde cada columna tiene un 1 (o un 0) que indica la pertenencia (o no) 
a cada una de las clases. A este proceso se lo llama one-hot-encoding."""

# one-hot encoding using keras' numpy-related utilities
y_train = y_train - 1
y_test = y_test - 1
n_classes = 2
print("Shape of y_train [before one-hot encoding]: ", y_train.shape)

# Generamos las nuevas variables que vamos a usar como etiquetas obejtivo
Y_train = np_utils.to_categorical(y_train, n_classes)
Y_test = np_utils.to_categorical(y_test, n_classes)
print("Shape of Y_train [after one-hot encoding]: ", Y_train.shape)

#%%
"""### Red Nueronal Simple
Para empezar, vamos a armar una red de una única capa (layer). Esto sería equivalente a tener un 10 percetrones con 784 inputs y un único output.  Cada perceptron está encargado de clasificar si la imagen pertenece o no a esa clase en particular. Verán que con keras esto es muy muy sencillo.

Primero, se define un objeto que es el modelo. A este objeto le iremos agregando layers que definiran cual es la arquitectura de la red (numero de unidades y numeros de capas). 

En esta instancia tambien vamos a definir el tipo de activación y el valor de regularización que vamos a utilizar.
"""

# Definimos el modelo secuencial (primero esta vacio)
model_simple = Sequential()

# Agregamos una capa, con activación softmnax
model_simple.add(Dense(2,activation='softmax'))

# Para analizar luego - comentar la anterior y descomentar esta linea
# model_simple.add(Dense(10, kernel_regularizer = l1(0.0005),activation='softmax'))

"""Luego se compila el modelo. En este paso se determina cual será la función de costo, con que métricas vamos a monitorear el proceso y cual será el optimizador (generalmente, algún tipo de descenso por gradiente).

Este problema de clasificación que estamos resolviendo es un una problema multi-clase donde las clases son mutuamente excluyentes (solo se peude pertenecer a una de las clases). En este tipo de problemas se debe usar en la ultima capa de la red el tipo de activación `softmax`, y se suele elegir la `categorical_crossentropy` como función de costo a optimizar.
"""

model_simple.compile(loss='categorical_crossentropy', metrics=['categorical_accuracy'], optimizer='adam')

"""Con todo esto definido, podemos entrenar la red."""

history = model_simple.fit(X_train, Y_train,
          batch_size=32, epochs=10,
          verbose=1,
          validation_data=(X_test, Y_test))

#%%
"""El modelo ya está entrenado! Ahora veamos que forma tien la salida del modelo. Para esto vamos a "pasar" la primer instancia del test set por el modelo y ver como es la salida:"""

salida = model_simple.predict(X_test[0:1])

print('Shape de la salida:',salida.shape)
print('Salida:',salida)

"""Lo que tenemos a la salida es la probabilidad de pertenecia a cada una de las clases. Tomaremos que la clase asignada por la red es aquella con mayor valor. En este caso:"""

ertiqueta_prediccion = salida[0].argmax()
etiquta_real = y_test[0]

print('Calse asignada por la red:', ertiqueta_prediccion)
print('Calse real:', etiquta_real)

plt.plot(salida[0:100].argmax())
plt.plot(y_test[0:100])

"""Para evaluar de manera generaal el resultado total vamos a calcular el accuracy sobre todo el set de entrenamiento y el todo el set de testeo:"""

# Usamos el modelo para predecir sobre todas las instancias en ambos sets
y_train_pred = model_simple.predict(X_train)
y_test_pred = model_simple.predict(X_test)

# Tomamos como clase predicha aquella con mayor probabilidad
train_accuracy =  metrics.accuracy_score(y_train_pred.argmax(axis=1),y_train)
test_accuracy =  metrics.accuracy_score(y_test_pred.argmax(axis=1), y_test)

print('Accuracy en el train set:', train_accuracy)
print('Accuracy en el test set:', test_accuracy)

"""Para inspeccionar un poco mejor los resultados, vamos a utilizar una función llamada `classification_report` (que computa distintas métricas sobre los datos) y vamos a calcular tambien la matrix de confusión. La clase que viene hablaremos de distitnas Métricas para evaluar un resultado."""

from sklearn.metrics import classification_report, confusion_matrix
print ("Classification Report")
print(classification_report(y_test, y_test_pred.argmax(axis=1)))
print ("Confusion Report")
print(confusion_matrix(y_test, y_test_pred.argmax(axis=1)))

"""#### ¿Qué está mirando la red?

Al tener una red simple de una sola capa, podemos evaluar con presición cuales son las carácterísticas de las imagenes que la red está pesando para decidir si una imagen corresponde a o no a determinada clase. Esta información esta codificada en el peso que la red le asigna a cada pixel.

Podemos indagar el valor aprendido por la red apra los pesos mediante el metodo `get_weights()`:
"""

weigths = model_simple.get_weights()[0]
print(weigths.shape)

"""Cada una de las 10 neuronas (uno por cada clase) tiene 784 pesos, uno por cada pixel de la imagen (sin contar los bias). La lectura del vector numérico de pesos no será muy informativa. Pero veamos que pasa si los llevamos a la forma `(28,28)` y los graficamos como una imagen.

"""

plt.figure(figsize = (15,8))
for i in range(2):
    plt.subplot(2,5,i+1)
    w0 = weigths[:,i]
    plt.title(categories[i])
    plt.imshow(w0.reshape(28,28))
plt.tight_layout()
plt.show()
#%%
"""La escala de colores nos indica la contribución positiva o negativa de 
cada uno de los pixeles para la desición de la neurona. Noten que aparecen 
sombras de las formas mas comunes apra cada tipo de producto. 
Tal vez el ejemplo mas evidente sea en la categoría de Pantalones, donde la 
neurona esta evaluando que no haya píxeles blancos entre las piernas.

**Ejercicios:** 
*   Vayan nuevamente a la celda donde definimos el modelo, y cambien la linea `model_simple.add()` que usamos por la que está comentada y vuelvan a correr todas las celdas. En este segundo caso, agregamos un tipo de regularización llamdo `l1` a los pesos del modelo (ya discutiremos esto en detalle mas adelante). 
*   Comparen el valor obtenido de acuraccy en el train y test set en ambos casos, y miren las matrices de los pesos. ¿Qué observación pueden hacer al respecto?
*   Observen la matriz de confusión y determine cuales fueron las clases mas dificiles de separar (las que el algoritmo mas se confunde). ¿Les parece razonable esto?

### Red Neuronal Profunda

Vamos resolver el mismo problema de clasificación, pero ahora con una red neuronal de varias layers.

Noten que las capas se puede agregar consecutivamente una atras de la otra. La cantidad de neuroans y capas que agreguemos es lo que va a determinar la arquitectura de nuestra red. Esto puede variar según el problema a resolver, pero lo importante es que la capa de entrada y la de salida tengan forma adecuada, es decir, que coincida con la de los datos de entremaniento. En nuestro caso, la capa entrada debe tener dimensión 784 y la salida uno de 10.
"""

# building a linear stack of layers with the sequential model
model = Sequential()
model.add(Dense(512, input_shape=(784,)))                        
#model.add(Dropout(0.2))

model.add(Dense(256, activation='relu'))
#model.add(Dropout(0.2))

model.add(Dense(128, activation='relu'))
#model.add(Dropout(0.2))

model.add(Dense(128, activation='relu'))
#model.add(Dropout(0.2))

model.add(Dense(2, activation='softmax'))

"""Nuevamente, una vez que defuinimos el modelo, debemos compilarlo."""

# compiling the sequential model
model.compile(loss='categorical_crossentropy', metrics=['categorical_accuracy'], optimizer='adam')

"""Para visualizar las características de nuestro modelo, podemos usar el método `.summary()`. Este nos muestra cual es la secuencia de capas que componen a nuestro modelo y la cantidad de unidades y parámetros que tiene cada una."""

model.summary()

plot_model(model)

"""Y ahora si, finalmente lo entrenamos con los datos."""

# training the model and saving metrics in history
history = model.fit(X_train, Y_train,
          batch_size=32, epochs=10,
          verbose=1,
          validation_data=(X_test, Y_test))

"""Este gráfico que vamos a hacer se llama curva de aprendizaje y es la manera clásica de graficar el proceso de entrenamiento en la red. Se trata del valor de alguna métrica (en nuestro caso la `accuracy`) y de la función de costo  (en nuestro caso la `categorical_crossentropy`) en función del número de epochs de entrenamiento."""

# plotting the metrics
fig = plt.figure(figsize = (10,8))
plt.subplot(2,1,1)
plt.plot(history.history['categorical_accuracy'])
plt.plot(history.history['val_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')

plt.subplot(2,1,2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')

plt.tight_layout()

"""Nuevamente podemos calcular de manera manual el accuraccy en el train y test set y armar el reporte de clasificación."""

# Usamos el modelo para predecir sobre todas las instancias en ambos sets
y_train_pred_NN = model.predict(X_train)
y_test_pred_NN = model.predict(X_test)

# Tomamos como clase predicha aquella con mayor probabilidad
train_accuracy =  metrics.accuracy_score(y_train_pred_NN.argmax(axis=1),y_train)
test_accuracy =  metrics.accuracy_score(y_test_pred_NN.argmax(axis=1), y_test)

print('Accuracy en el train set:', train_accuracy)
print('Accuracy en el test set:', test_accuracy)

print ("Classification Report")
print(classification_report(y_test, y_test_pred_NN.argmax(axis=1)))
print ("Confusion Report")
print(confusion_matrix(y_test, y_test_pred_NN.argmax(axis=1)))

"""**Ejercicios:** 
*   Vayan nuevamente a la celda donde definimos el modelo, descomenten las lineas que dicen `model.add(Dropout(0.2))` y vuelvan a correr todas las celdas. En este segundo caso, agregamos una estrategia de regularización llamda dropout (ya discutiremos esto en detalle mas adelante). 
*   Comparen el valor obtenido de acuraccy y loss en el train y test set en ambos casos ¿Qué observación pueden hacer al respecto?
*   Prueben distintas arquitecturas de red.
"""
# %%
